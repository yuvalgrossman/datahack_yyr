{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from urllib import request\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, Binarizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>load data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ran/Dev/dh_yyr/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (24,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train_accounts = pd.read_csv(data_path + 'train_accounts.csv')\n",
    "# train_users = pd.read_csv(data_path + 'train_users.csv')\n",
    "# train_events = pd.read_csv(data_path + 'train_events.csv')\n",
    "# train_subscriptions = pd.read_csv(data_path + 'train_subscriptions.csv')\n",
    "test_accounts = pd.read_csv(data_path + 'test_accounts.csv')\n",
    "# test_users = pd.read_csv(data_path + 'test_users.csv')\n",
    "# test_events = pd.read_csv(data_path + 'test_events.csv')\n",
    "# test_subscriptions = pd.read_csv(data_path + 'test_subscriptions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>feature engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at\n",
    "churn date\n",
    "user role\n",
    "domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_in[df_in[col_name] > fence_high] = fence_high\n",
    "    df_in[df_in[col_name] < fence_low] = fence_low\n",
    "    #return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform plan_id & utm_cluster_id to str since its categorical\n",
    "train_accounts['plan_id'] = train_accounts['plan_id'].astype(str)\n",
    "train_accounts['utm_cluster_id'] = train_accounts['utm_cluster_id'].astype(str)\n",
    "test_accounts['plan_id'] = test_accounts['plan_id'].astype(str)\n",
    "test_accounts['utm_cluster_id'] = test_accounts['utm_cluster_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating time features\n",
    "time_between_created_trial = pd.to_datetime(train_accounts['trial_start']) - pd.to_datetime(train_accounts['created_at'])\n",
    "time_between_created_subscription = pd.to_datetime(train_accounts['subscription_started_at']) - pd.to_datetime(train_accounts['created_at'])\n",
    "time_between_trial_subscription = pd.to_datetime(train_accounts['subscription_started_at']) - pd.to_datetime(train_accounts['trial_start'])\n",
    "time_between_now_trial = datetime.now() - pd.to_datetime(train_accounts['trial_start'])\n",
    "time_between_now_subscription = datetime.now() - pd.to_datetime(train_accounts['subscription_started_at'])\n",
    "time_between_now_created = datetime.now() - pd.to_datetime(train_accounts['created_at'])\n",
    "train_accounts = train_accounts.assign(created_trial_delta=time_between_created_trial.apply(lambda x: (x.seconds//3600)))\n",
    "train_accounts = train_accounts.assign(created_subscription_delta=time_between_created_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "train_accounts = train_accounts.assign(trial_subscription_delta=time_between_trial_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "train_accounts = train_accounts.assign(now_trial_delta=time_between_now_trial.apply(lambda x: (x.seconds//3600)))\n",
    "train_accounts = train_accounts.assign(now_subscription_delta=time_between_now_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "train_accounts = train_accounts.assign(now_created_delta=time_between_now_created.apply(lambda x: (x.seconds//3600)))\n",
    "train_accounts['is_subscription'] = (train_accounts.subscription_started_at.isna()).astype(int)\n",
    "\n",
    "time_between_created_trial = pd.to_datetime(test_accounts['trial_start']) - pd.to_datetime(test_accounts['created_at'])\n",
    "time_between_created_subscription = pd.to_datetime(test_accounts['subscription_started_at']) - pd.to_datetime(test_accounts['created_at'])\n",
    "time_between_trial_subscription = pd.to_datetime(test_accounts['subscription_started_at']) - pd.to_datetime(test_accounts['trial_start'])\n",
    "time_between_now_trial = datetime.now() - pd.to_datetime(test_accounts['trial_start'])\n",
    "time_between_now_subscription = datetime.now() - pd.to_datetime(test_accounts['subscription_started_at'])\n",
    "time_between_now_created = datetime.now() - pd.to_datetime(test_accounts['created_at'])\n",
    "test_accounts = test_accounts.assign(created_trial_delta=time_between_created_trial.apply(lambda x: (x.seconds//3600)))\n",
    "test_accounts = test_accounts.assign(created_subscription_delta=time_between_created_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "test_accounts = test_accounts.assign(trial_subscription_delta=time_between_trial_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "test_accounts = test_accounts.assign(now_trial_delta=time_between_now_trial.apply(lambda x: (x.seconds//3600)))\n",
    "test_accounts = test_accounts.assign(now_subscription_delta=time_between_now_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "test_accounts = test_accounts.assign(now_created_delta=time_between_now_created.apply(lambda x: (x.seconds//3600)))\n",
    "test_accounts['is_subscription'] = (test_accounts.subscription_started_at.isna()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating size & survey features\n",
    "bins = sorted((list(train_accounts[\"max_team_size\"].value_counts().index) + [-1.1, -1, ]))\n",
    "bins_labels = [str(b) for b in bins[1:]]\n",
    "\n",
    "clip_outlier(train_accounts,'company_size')\n",
    "train_accounts.loc[:,'avg_team_size'] = train_accounts[[\"min_team_size\", \"max_team_size\"]].mean(axis=1)\n",
    "train_accounts['avg_team_size'].fillna(-1, inplace=True)\n",
    "train_accounts['avg_team_cat'] = pd.cut(train_accounts['avg_team_size'], bins=bins, labels=bins_labels)\n",
    "train_accounts['avg_team_cat'] = train_accounts['avg_team_cat'].astype(str)\n",
    "train_accounts['survey_answers'] = train_accounts[['company_size','max_team_size','min_team_size','user_goal','user_description','team_size']].isna().sum(axis=1)\n",
    "train_accounts['survey_did_answer'] = train_accounts['survey_answers']\n",
    "\n",
    "clip_outlier(test_accounts,'company_size')\n",
    "test_accounts.loc[:,'avg_team_size'] = test_accounts[[\"min_team_size\", \"max_team_size\"]].mean(axis=1)\n",
    "test_accounts['avg_team_size'].fillna(-1, inplace=True)\n",
    "test_accounts['survey_answers'] = test_accounts[['company_size','max_team_size','min_team_size','user_goal','user_description','team_size']].isna().sum(axis=1)\n",
    "test_accounts['survey_did_answer'] = test_accounts['survey_answers']\n",
    "test_accounts['avg_team_cat'] = pd.cut(test_accounts['avg_team_size'], bins=bins, labels=bins_labels)\n",
    "test_accounts['avg_team_cat'] = test_accounts['avg_team_cat'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We map our features into different types\n",
    "categorical_features = ['os', 'browser', 'payment_currency', 'device', 'country', 'industry', 'utm_cluster_id',\n",
    "                        'plan_id', 'avg_team_cat']\n",
    "normalized_features = ['collection_21_days', 'mrr', 'created_trial_delta', 'created_subscription_delta',\n",
    "                       'trial_subscription_delta', 'now_trial_delta', 'now_subscription_delta', 'now_created_delta',\n",
    "                       'company_size', 'survey_answers']\n",
    "binary_features = ['survey_did_answer']\n",
    "untouched_features = ['paying', 'is_subscription']\n",
    "KBinsDiscretized_features = []\n",
    "target = ['lead_score']\n",
    "\n",
    "# And create a column transformer to handle the manipulation for us\n",
    "preprocess = make_column_transformer(\n",
    "    (OneHotEncoder(), categorical_features),\n",
    "    (Normalizer(), normalized_features),\n",
    "    (Binarizer(), binary_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ran/Dev/dh_yyr/venv/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "train_accounts.set_index('account_id', inplace=True)\n",
    "test_accounts.set_index('account_id', inplace=True)\n",
    "\n",
    "# Getting only the relevant features from the dataset\n",
    "dataset_train = train_accounts[categorical_features + normalized_features + binary_features + untouched_features + target]\n",
    "dataset_test = test_accounts[categorical_features + normalized_features + binary_features + untouched_features]\n",
    "\n",
    "# Filling empty values with default values \n",
    "def fill_empty_values(dataset):\n",
    "    dataset.loc[:,categorical_features] = dataset[categorical_features].fillna('')\n",
    "    dataset.loc[:,normalized_features + binary_features + untouched_features] = dataset[normalized_features + binary_features + untouched_features].fillna(0)\n",
    "    return dataset\n",
    "\n",
    "dataset_train = fill_empty_values(dataset_train)\n",
    "dataset_test = fill_empty_values(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the label\n",
    "y = dataset_train.pop('lead_score')\n",
    "# We fit our column transformer on both the train and the test sets\n",
    "concatenated = pd.concat([dataset_train, dataset_test])\n",
    "preprocess.fit(concatenated)\n",
    "\n",
    "# We use transform to finally manipulate the features of our training set\n",
    "dataset_train = dataset_train[concatenated.columns]\n",
    "x = preprocess.transform(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>train model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ran/Dev/dh_yyr/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ran/Dev/dh_yyr/venv/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=42)\n",
    "model = LogisticRegression(class_weight='balanced', penalty='l1') # 'penalty': ['l1', 'l2'], 'C': [1, 10, 100, 1000]\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not lead       0.98      0.74      0.84     10669\n",
      "        lead       0.25      0.86      0.38      1064\n",
      "\n",
      "    accuracy                           0.75     11733\n",
      "   macro avg       0.61      0.80      0.61     11733\n",
      "weighted avg       0.91      0.75      0.80     11733\n",
      "\n",
      "Acc:  0.7492542401772777\n",
      "MCC: 0.3689924800420734\n",
      "F1:  0.38322851153039833\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['not lead','lead']))\n",
    "print('Acc:  {}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print('MCC: {}'.format(metrics.matthews_corrcoef(y_test, y_pred)))\n",
    "print('F1:  {}'.format(metrics.f1_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>submit</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset_test[concatenated.columns]\n",
    "x_submission = preprocess.transform(dataset_test)\n",
    "y_pred_submission = model.predict(x_submission)\n",
    "# Creating a dictionary where the keys are the account_ids\n",
    "# and the values are your predictions\n",
    "submission_account_ids = test_accounts.index\n",
    "predictions = dict(zip(submission_account_ids, map(int, y_pred_submission)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'fRidaY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "You have to send all of the accounts! Expected: (71683, 71683), Got: (12462, 12462)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c405c905e421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We validate first that we actually send all the test accounts expected to be sent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_pred_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m71683\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msubmission_account_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m71683\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to send all of the accounts! Expected: (71683, 71683), Got: ({}, {})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmission_account_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"group_name\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgroup_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: You have to send all of the accounts! Expected: (71683, 71683), Got: (12462, 12462)"
     ]
    }
   ],
   "source": [
    "# We validate first that we actually send all the test accounts expected to be sent\n",
    "if y_pred_submission.shape[0] != 71683 or submission_account_ids.shape[0] != 71683:\n",
    "  raise Exception(\"You have to send all of the accounts! Expected: (71683, 71683), Got: ({}, {})\".format(y_pred_submission.shape[0], submission_account_ids.shape[0]))\n",
    "\n",
    "if \"group_name\" not in vars() or group_name == \"\":\n",
    "  group_name = input(\"Please enter your group's name:\")\n",
    "\n",
    "data = json.dumps({'submitter': group_name, 'predictions': predictions}).encode('utf-8')\n",
    "\n",
    "req = request.Request(\"https://leaderboard.datahack.org.il/monday/api/\",\n",
    "                      headers={'Content-Type': 'application/json'},\n",
    "                      data=data)\n",
    "\n",
    "res = request.urlopen(req)\n",
    "print(json.load(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(predictions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

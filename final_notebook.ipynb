{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from urllib import request\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, Binarizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>load data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accounts = pd.read_csv(data_path + 'train_accounts.csv')\n",
    "# train_users = pd.read_csv(data_path + 'train_users.csv')\n",
    "# train_events = pd.read_csv(data_path + 'train_events.csv')\n",
    "# train_subscriptions = pd.read_csv(data_path + 'train_subscriptions.csv')\n",
    "test_accounts = pd.read_csv(data_path + 'test_accounts.csv')\n",
    "# test_users = pd.read_csv(data_path + 'test_users.csv')\n",
    "# test_events = pd.read_csv(data_path + 'test_events.csv')\n",
    "# test_subscriptions = pd.read_csv(data_path + 'test_subscriptions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>feature engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at\n",
    "churn date\n",
    "user role\n",
    "domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform plan_id & utm_cluster_id to str since its categorical\n",
    "train_accounts['plan_id'] = train_accounts['plan_id'].astype(str)\n",
    "train_accounts['utm_cluster_id'] = train_accounts['utm_cluster_id'].astype(str)\n",
    "test_accounts['plan_id'] = test_accounts['plan_id'].astype(str)\n",
    "test_accounts['utm_cluster_id'] = test_accounts['utm_cluster_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_in.loc[df_in[col_name] > fence_high, col_name] = fence_high\n",
    "    df_in.loc[df_in[col_name] < fence_low, col_name] = fence_low\n",
    "    \n",
    "def creating_time_features(data):\n",
    "    time_between_created_trial = pd.to_datetime(data['trial_start']) - pd.to_datetime(data['created_at'])\n",
    "    time_between_created_subscription = pd.to_datetime(data['subscription_started_at']) - pd.to_datetime(data['created_at'])\n",
    "    time_between_trial_subscription = pd.to_datetime(data['subscription_started_at']) - pd.to_datetime(data['trial_start'])\n",
    "    time_between_now_trial = datetime.now() - pd.to_datetime(data['trial_start'])\n",
    "    time_between_now_subscription = datetime.now() - pd.to_datetime(data['subscription_started_at'])\n",
    "    time_between_now_created = datetime.now() - pd.to_datetime(data['created_at'])\n",
    "    time_between_now_churn = datetime.now() - pd.to_datetime(data['churn_date'])\n",
    "    time_between_churn_subscription = pd.to_datetime(data['churn_date']) - pd.to_datetime(data['subscription_started_at'])\n",
    "\n",
    "    data = data.assign(created_trial_delta=time_between_created_trial.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(created_subscription_delta=time_between_created_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(trial_subscription_delta=time_between_trial_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_trial_delta=time_between_now_trial.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_subscription_delta=time_between_now_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_created_delta=time_between_now_created.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_churn_delta=time_between_now_churn.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(churn_subscription_delta=time_between_churn_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data['is_subscription'] = (data.subscription_started_at.isna()).astype(int)\n",
    "    data['is_churn'] = (data.churn_date.isna()).astype(int)\n",
    "    return data\n",
    "\n",
    "def creating_size_n_survey_features(data, bins, bins_labels):\n",
    "    #clip_outlier(data,'company_size')\n",
    "    data.loc[:,'avg_team_size'] = data[[\"min_team_size\", \"max_team_size\"]].mean(axis=1)\n",
    "    data['avg_team_size'].fillna(-1, inplace=True)\n",
    "    data['avg_team_cat'] = pd.cut(data['avg_team_size'], bins=bins, labels=bins_labels)\n",
    "    data['avg_team_cat'] = data['avg_team_cat'].astype(str)\n",
    "    data['survey_answers'] = data[['company_size','max_team_size','min_team_size','user_goal','user_description','team_size']].isna().sum(axis=1)\n",
    "    data['survey_did_answer'] = data['survey_answers']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = pd.concat([train_accounts, test_accounts])\n",
    "x['country_counts'] = x.groupby('country')['country'].transform('count')\n",
    "x['region_counts'] = x.groupby('region')['region'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating size & survey features\n",
    "bins = sorted((list(train_accounts[\"max_team_size\"].value_counts().index) + [-1.1, -1, ]))\n",
    "bins_labels = [str(b) for b in bins[1:]]\n",
    "\n",
    "train_accounts = creating_time_features(train_accounts)\n",
    "train_accounts = creating_size_n_survey_features(train_accounts, bins, bins_labels)\n",
    "test_accounts = creating_time_features(test_accounts)\n",
    "test_accounts = creating_size_n_survey_features(test_accounts, bins, bins_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", 'avg_team_cat'\n",
    "'now_churn_delta', 'churn_subscription_delta', 'company_size', 'survey_answers'\n",
    ", 'is_subscription', 'is_churn'\n",
    "'survey_did_answer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We map our features into different types\n",
    "categorical_features = ['os', 'browser', 'payment_currency', 'device', 'country', 'industry', 'utm_cluster_id',\n",
    "                         'plan_id', 'avg_team_cat']\n",
    "normalized_features = ['collection_21_days', 'mrr', 'created_trial_delta', 'created_subscription_delta',\n",
    "                       'trial_subscription_delta', 'now_trial_delta', 'now_subscription_delta', 'now_created_delta', \n",
    "                       'now_churn_delta', 'churn_subscription_delta', 'company_size', 'survey_answers']\n",
    "normalized_features = []\n",
    "binary_features = ['survey_did_answer']\n",
    "untouched_features = ['paying', 'is_subscription', 'is_churn']\n",
    "KBinsDiscretized_features = []\n",
    "target = ['lead_score']\n",
    "\n",
    "# And create a column transformer to handle the manipulation for us\n",
    "preprocess = make_column_transformer(\n",
    "    (OneHotEncoder(), categorical_features),\n",
    "    (Normalizer(), normalized_features),\n",
    "    (Binarizer(), binary_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ran/Dev/dh_yyr/venv/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "if 'account_id' in train_accounts.columns:\n",
    "    train_accounts.set_index('account_id', inplace=True)\n",
    "    test_accounts.set_index('account_id', inplace=True)\n",
    "\n",
    "# Getting only the relevant features from the dataset\n",
    "dataset_train = train_accounts[categorical_features + normalized_features + binary_features + untouched_features + target]\n",
    "dataset_test = test_accounts[categorical_features + normalized_features + binary_features + untouched_features]\n",
    "\n",
    "# Filling empty values with default values \n",
    "def fill_empty_values(dataset):\n",
    "    dataset.loc[:,categorical_features] = dataset[categorical_features].fillna('')\n",
    "    dataset.loc[:,normalized_features + binary_features + untouched_features] = dataset[normalized_features + binary_features + untouched_features].fillna(0)\n",
    "    return dataset\n",
    "\n",
    "dataset_train = fill_empty_values(dataset_train)\n",
    "dataset_test = fill_empty_values(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the label\n",
    "y = dataset_train.pop('lead_score')\n",
    "# We fit our column transformer on both the train and the test sets\n",
    "concatenated = pd.concat([dataset_train, dataset_test])\n",
    "preprocess.fit(concatenated)\n",
    "\n",
    "# We use transform to finally manipulate the features of our training set\n",
    "dataset_train = dataset_train[concatenated.columns]\n",
    "x = preprocess.transform(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>train model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ran/Dev/dh_yyr/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ran/Dev/dh_yyr/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1544: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=42)\n",
    "model = LogisticRegression(class_weight='balanced', penalty='l1', n_jobs=-1) # 'penalty': ['l1', 'l2'], 'C': [1, 10, 100, 1000]\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not lead       0.99      0.79      0.88     66427\n",
      "        lead       0.09      0.79      0.16      1672\n",
      "\n",
      "    accuracy                           0.79     68099\n",
      "   macro avg       0.54      0.79      0.52     68099\n",
      "weighted avg       0.97      0.79      0.86     68099\n",
      "\n",
      "Acc:  0.7930366084670847\n",
      "MCC: 0.21909805310133137\n",
      "F1:  0.15866762177650431\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['not lead','lead']))\n",
    "print('Acc:  {}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print('MCC: {}'.format(metrics.matthews_corrcoef(y_test, y_pred)))\n",
    "print('F1:  {}'.format(metrics.f1_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>submit</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = dataset_test[concatenated.columns]\n",
    "x_submission = preprocess.transform(dataset_test)\n",
    "y_pred_submission = model.predict(x_submission)\n",
    "# Creating a dictionary where the keys are the account_ids\n",
    "# and the values are your predictions\n",
    "submission_account_ids = [str(int(i))for i in test_accounts.index]\n",
    "predictions = dict(zip(submission_account_ids, map(int, y_pred_submission)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'fRidaY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 'fRidaY', 'rank': 2, 'score': 0.22036695149604393}\n"
     ]
    }
   ],
   "source": [
    "# We validate first that we actually send all the test accounts expected to be sent\n",
    "if y_pred_submission.shape[0] != 71683 or len(submission_account_ids) != 71683:\n",
    "  raise Exception(\"You have to send all of the accounts! Expected: (71683, 71683), Got: ({}, {})\".format(y_pred_submission.shape[0], submission_account_ids.shape[0]))\n",
    "\n",
    "if \"group_name\" not in vars() or group_name == \"\":\n",
    "  group_name = input(\"Please enter your group's name:\")\n",
    "\n",
    "data = json.dumps({'submitter': group_name, 'predictions': predictions}).encode('utf-8')\n",
    "\n",
    "req = request.Request(\"https://leaderboard.datahack.org.il/monday/api/\",\n",
    "                      headers={'Content-Type': 'application/json'},\n",
    "                      data=data)\n",
    "\n",
    "res = request.urlopen(req)\n",
    "print(json.load(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(predictions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

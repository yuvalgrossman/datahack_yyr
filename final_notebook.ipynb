{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download (snippet from: https://colab.research.google.com/drive/1_Y-sZ5eHIDlDUMuLCwfnbuJdLh0DTXmO#scrollTo=5HGlaJTEAYJu&line=23&uniqifier=1)\n",
    "import os\n",
    "\n",
    "# We define the datasets we want to load\n",
    "datasets = ('accounts', 'users', 'subscriptions', 'events')\n",
    "source_prefix = 'https://storage.googleapis.com/mondaycom-datahack/final_sets/'\n",
    "\n",
    "local_dir = './data/'\n",
    "file_prefixs = ['train_','test_']\n",
    "file_suffix = ''\n",
    "file_extension = 'csv'\n",
    "\n",
    "# We create a directory for the datasets if it doesn't exist\n",
    "if not os.path.exists(local_dir):\n",
    "    os.makedirs(local_dir)\n",
    "\n",
    "for file_prefix in file_prefixs:\n",
    "    # For each dataset we want, we check if we already downloaded it and fix it if we didn't\n",
    "    for dataset in datasets:\n",
    "      if not os.path.isfile('{}{}{}{}.{}'.format(local_dir, file_prefix, dataset, file_suffix, file_extension)):\n",
    "        !curl {source_prefix}{file_prefix}{dataset}{file_suffix}.{file_extension} --output {local_dir}{file_prefix}{dataset}{file_suffix}.{file_extension}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import modin.pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib import request\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, Binarizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "# print each command: \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# from multiprocessing import  Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>load data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4c265ad964cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_accounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_accounts.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.sample(frac=0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_users.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_events.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_subscriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_subscriptions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/modin/pandas/io.py\u001b[0m in \u001b[0;36mparser_func\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/modin/pandas/io.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mKeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mpd_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseFactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;31m# This happens when `read_csv` returns a TextFileReader object for iterating through\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextFileReader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/modin/data_management/factories.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/modin/data_management/factories.py\u001b[0m in \u001b[0;36m_read_csv\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/modin/engines/ray/generic/io.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(cls, filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv_remote_task\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRayIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/modin/engines/ray/generic/io.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(cls, filepath_or_buffer, **kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_csv_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_csv_from_file_ray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/modin/engines/ray/generic/io.py\u001b[0m in \u001b[0;36m_read_csv_from_file_ray\u001b[0;34m(cls, filepath, kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# or based on the column(s) that were requested.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mrow_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRangeIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_ids)\u001b[0m\n\u001b[1;32m   2241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mlast_task_error_raise_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2243\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2244\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2245\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget_object\u001b[0;34m(self, object_ids)\u001b[0m\n\u001b[1;32m    585\u001b[0m                     max([\n\u001b[1;32m    586\u001b[0m                         \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_timeout_milliseconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m                         \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munready_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m                     ]),\n\u001b[1;32m    589\u001b[0m                 )\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mretrieve_and_deserialize\u001b[0;34m(self, object_ids, timeout, error_timeout)\u001b[0m\n\u001b[1;32m    455\u001b[0m                         \u001b[0mobject_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                         \u001b[0mwith_meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m                     )\n\u001b[1;32m    459\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_data_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/ray/utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0morig_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapper_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/ray/pyarrow_files/pyarrow/_plasma.pyx\u001b[0m in \u001b[0;36mpyarrow._plasma.PlasmaClient.get_buffers\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/ray/pyarrow_files/pyarrow/_plasma.pyx\u001b[0m in \u001b[0;36mpyarrow._plasma.PlasmaClient._get_object_buffers\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/ray/pyarrow_files/pyarrow/_plasma.pyx\u001b[0m in \u001b[0;36mpyarrow._plasma.plasma_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/datahack_yyr/venv/lib/python3.7/site-packages/ray/pyarrow_files/pyarrow/compat.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(o)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "train_accounts = pd.read_csv(data_path + 'train_accounts.csv')#.sample(frac=0.2)\n",
    "train_users = pd.read_csv(data_path + 'train_users.csv')\n",
    "train_events = pd.read_csv(data_path + 'train_events.csv')\n",
    "train_subscriptions = pd.read_csv(data_path + 'train_subscriptions.csv')\n",
    "test_accounts = pd.read_csv(data_path + 'test_accounts.csv')\n",
    "test_users = pd.read_csv(data_path + 'test_users.csv')\n",
    "test_events = pd.read_csv(data_path + 'test_events.csv')\n",
    "test_subscriptions = pd.read_csv(data_path + 'test_subscriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "accounts = pd.concat([train_accounts, test_accounts],sort=False)\n",
    "print(f'accounts: {len(accounts)}')\n",
    "users = pd.concat([train_users, test_users],sort=False)\n",
    "print(f'users: {len(users)}')\n",
    "events = pd.concat([train_events, test_events],sort=False)\n",
    "print(f'events: {len(events)}')\n",
    "subscriptions = pd.concat([train_subscriptions, test_subscriptions],sort=False)\n",
    "print(f'subscriptions: {len(subscriptions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dask df: \n",
    "# accountsd=dd.from_pandas(accounts,npartitions=1)\n",
    "# usersd=dd.from_pandas(users,npartitions=1)\n",
    "# eventsd=dd.from_pandas(events,npartitions=1)\n",
    "# subscriptions=dd.from_pandas(subscriptions,npartitions=1)\n",
    "# dask.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>feature engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## direct numerical correlation\n",
    "First lets exampain the correlation between all numerical columns in \"accounts\" and the target columns \"lead_score\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = accounts.select_dtypes('number').corr()\n",
    "f, ax = plt.subplots(figsize = (11,9))\n",
    "corrmat['lead_score'][0:-1].plot(kind='bar',rot=60)\n",
    "plt.title('Correlation with lead_score')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like the best features would be \"company_size\" and \"mmr\", and maybe also \"paying\", \"collection_21_days\" and \"time_diff\". \n",
    "\n",
    "Next, we examine numerical features from the other 3 tables. The approach here is to calculate the sum (and other several statistics) of each columns for each account, and then find which of them has correlation with the target: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_above(df,df_name,d):\n",
    "    n_df = df.groupby('account_id')[df.columns[1]].count()\n",
    "    idx1 = n_df.iloc[n_df.values<=d]\n",
    "    idx2 = n_df.iloc[n_df.values>d]\n",
    "    print('{} accounts with more than {} {}.'.format(len(idx2),d,df_name))\n",
    "    return idx1, idx2\n",
    "idx1_us, idx2_us = groupby_above(events,'events',500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # tnames = ['users','events','subscriptions']\n",
    "# # for i,table in enumerate([users,events,subscriptions]):\n",
    "\n",
    "def extract_num_feat(df):\n",
    "    cols=df.select_dtypes('number').columns # columns names\n",
    "    n_df1 = accounts.set_index('account_id').loc[:,['lead_score']] #new dataframe with account as index and columns lead_score\n",
    "    n_df2 = df.groupby('account_id').filter(lambda x: x[df.columns[1]].count()>500).groupby('account_id')[cols].describe()\n",
    "    n_df = n_df2.join(n_df1)\n",
    "#     return n_df,n_df1,n_df2\n",
    "    return n_df\n",
    "\n",
    "df_us = extract_num_feat(events)\n",
    "# len(idx1_us)+len(idx2_us)\n",
    "df_us.to_csv('accounts_above500events_feat.csv')\n",
    "df_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_users.iloc[(test_users.groupby('account_id')['user_id'].count()>1).values]\n",
    "# test_users.where\n",
    "gbidx=test_users.groupby('account_id').filter(lambda x: x['user_id'].count()>10).groupby('account_id').count()\n",
    "gbidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A':np.random.randint(-10,10,1000000),'B':np.random.random(1000000)})\n",
    "df1 = df.groupby('A').filter(lambda x: x['A'].mean()>0).groupby('A').count()\n",
    "df1\n",
    "df2 = df[df.A >= 0].groupby('A').count()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=test_users.groupby('account_id')\n",
    "# b=a.iloc[idx2_us].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=(lambda x: x['user_id'].count()>10)\n",
    "# x1\n",
    "df= test_users\n",
    "# x1=(df.groupby('account_id')[df.columns[1]].count()>1)\n",
    "# x1.values\n",
    "a.filter(x1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.nth(list(idx2_us.index)).count()\n",
    "a.iloc[x1.values].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are more accounts in 'events' than in 'accounts': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events['account_id'].max()\n",
    "accounts['account_id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_num_feat(df):\n",
    "    cols=df.select_dtypes('number').columns # columns names\n",
    "    n_df1 = accounts.set_index('account_id').loc[:,['lead_score']] #new dataframe with account as index and columns lead_score\n",
    "    #     for col in cols: #create features for each column\n",
    "    # col_names = [col+'_'+stat for i,col in enumerate(cols)]\n",
    "    n_df2 = df[cols].describe()\n",
    "    n_df = n_df2.join(n_df1)\n",
    "#     return n_df,n_df1,n_df2\n",
    "    return n_df\n",
    "       \n",
    "st=time.time()\n",
    "# ex_feat_ev,ev1,ev2 = extract_num_feat(events.iloc[idx_ev])\n",
    "# print(time.time()-st)\n",
    "ex_feat_us,us1,us2 = extract_num_feat(users.iloc[idx_us])\n",
    "print(time.time()-st)\n",
    "# # ex_feat_sb,sb1,sb2 = extract_num_feat(subscriptions.iloc[0:1000])\n",
    "# print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ex_feat_us,us1,us2=parallelize_dataframe(users.iloc[0:10000], extract_num_feat, n_cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for ex_feat in list([ex_feat_ev,ex_feat_us]):\n",
    "    corrs = ex_feat.select_dtypes('number').corrwith(ex_feat.lead_score)\n",
    "    f, ax = plt.subplots(figsize = (11,9))\n",
    "    print(corrs[corrs.abs().values>0.06][:-1])\n",
    "    corrs[corrs.abs().values>0.07][:-1].plot(kind='bar',rot=60)\n",
    "    plt.title('Correlation with lead_score')\n",
    "    plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs[corrs.abs().values>0.05][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at\n",
    "churn date\n",
    "user role\n",
    "domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform plan_id & utm_cluster_id to str since its categorical\n",
    "accounts['plan_id'] = accounts['plan_id'].astype(str)\n",
    "accounts['utm_cluster_id'] = accounts['utm_cluster_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_in.loc[df_in[col_name] > fence_high, col_name] = fence_high\n",
    "    df_in.loc[df_in[col_name] < fence_low, col_name] = fence_low\n",
    "    \n",
    "def creating_time_features(data):\n",
    "    time_between_created_trial = pd.to_datetime(data['trial_start']) - pd.to_datetime(data['created_at'])\n",
    "    time_between_created_subscription = pd.to_datetime(data['subscription_started_at']) - pd.to_datetime(data['created_at'])\n",
    "    time_between_trial_subscription = pd.to_datetime(data['subscription_started_at']) - pd.to_datetime(data['trial_start'])\n",
    "    time_between_now_trial = datetime.now() - pd.to_datetime(data['trial_start'])\n",
    "    time_between_now_subscription = datetime.now() - pd.to_datetime(data['subscription_started_at'])\n",
    "    time_between_now_created = datetime.now() - pd.to_datetime(data['created_at'])\n",
    "    time_between_now_churn = datetime.now() - pd.to_datetime(data['churn_date'])\n",
    "    time_between_churn_subscription = pd.to_datetime(data['churn_date']) - pd.to_datetime(data['subscription_started_at'])\n",
    "\n",
    "    data = data.assign(created_trial_delta=time_between_created_trial.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(created_subscription_delta=time_between_created_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(trial_subscription_delta=time_between_trial_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_trial_delta=time_between_now_trial.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_subscription_delta=time_between_now_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_created_delta=time_between_now_created.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_churn_delta=time_between_now_churn.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(churn_subscription_delta=time_between_churn_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data['is_subscription'] = (data.subscription_started_at.isna()).astype(int)\n",
    "    data['is_churn'] = (data.churn_date.isna()).astype(int)\n",
    "    return data\n",
    "\n",
    "def creating_size_n_survey_features(data, bins, bins_labels):\n",
    "    #clip_outlier(data,'company_size')\n",
    "    data.loc[:,'avg_team_size'] = data[[\"min_team_size\", \"max_team_size\"]].mean(axis=1)\n",
    "    data['avg_team_size'].fillna(-1, inplace=True)\n",
    "    data['avg_team_cat'] = pd.cut(data['avg_team_size'], bins=bins, labels=bins_labels)\n",
    "    data['avg_team_cat'] = data['avg_team_cat'].astype(str)\n",
    "    data['survey_answers'] = data[['company_size','max_team_size','min_team_size','user_goal','user_description','team_size']].isna().sum(axis=1)\n",
    "    data['survey_did_answer'] = data['survey_answers']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating size & survey features\n",
    "bins = sorted((list(train_accounts[\"max_team_size\"].value_counts().index) + [-1.1, -1, ]))\n",
    "bins_labels = [str(b) for b in bins[1:]]\n",
    "\n",
    "accounts = creating_time_features(accounts)\n",
    "accounts = creating_size_n_survey_features(accounts, bins, bins_labels)\n",
    "accounts['country_counts'] = accounts.groupby('country')['country'].transform('count')\n",
    "accounts['region_counts'] = accounts.groupby('region')['region'].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We map our features into different types\n",
    "categorical_features = ['os', 'browser', 'payment_currency', 'device', 'industry', 'utm_cluster_id',\n",
    "                         'plan_id', 'avg_team_cat']\n",
    "normalized_features = ['collection_21_days', 'mrr', 'created_trial_delta', 'created_subscription_delta',\n",
    "                       'trial_subscription_delta', 'now_trial_delta', 'now_subscription_delta', 'now_created_delta', \n",
    "                       'now_churn_delta', 'churn_subscription_delta', 'company_size', 'survey_answers']\n",
    "\n",
    "binary_features = ['survey_did_answer']\n",
    "untouched_features = ['paying', 'is_subscription', 'is_churn']\n",
    "KBinsDiscretized_features = []\n",
    "target = ['lead_score']\n",
    "\n",
    "# And create a column transformer to handle the manipulation for us\n",
    "preprocess = make_column_transformer(\n",
    "    (OneHotEncoder(), categorical_features),\n",
    "    (Normalizer(), normalized_features),\n",
    "    (Binarizer(), binary_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'account_id' in accounts.columns:\n",
    "    accounts.set_index('account_id', inplace=True)\n",
    "    #test_accounts.set_index('account_id', inplace=True)\n",
    "\n",
    "    # Filling empty values with default values \n",
    "def fill_empty_values(dataset):\n",
    "    dataset.loc[:,categorical_features] = dataset[categorical_features].fillna('')\n",
    "    dataset.loc[:,normalized_features + binary_features + untouched_features] = dataset[normalized_features + binary_features + untouched_features].fillna(0)\n",
    "    return dataset\n",
    "\n",
    "accounts = fill_empty_values(accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sample(data, target, neg_pos_ratio=1):\n",
    "    target_num = len(data[data[target] == 1])\n",
    "    negative_idx = data[data[target] == 0].index\n",
    "    positve_idx = data[data[target] == 1].index\n",
    "    rnd_negative = np.random.choice(negative_idx , target_num * neg_pos_ratio, replace=False)\n",
    "    under_sample_idx = np.concatenate([positve_idx, rnd_negative])\n",
    "    return data.loc[under_sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit our column transformer on both the train and the test sets\n",
    "preprocess.fit(accounts.drop('lead_score',axis=1))\n",
    "\n",
    "#dataset_train = accounts.loc[train_accounts.account_id]\n",
    "dataset_train = under_sample(accounts.loc[train_accounts.account_id], 'lead_score', 20)\n",
    "dataset_test = accounts.loc[test_accounts.account_id].drop('lead_score',axis=1)\n",
    "\n",
    "# We use transform to finally manipulate the features of our training set\n",
    "x = preprocess.transform(dataset_train.drop('lead_score',axis=1))\n",
    "# Seperating the label\n",
    "y = dataset_train['lead_score']\n",
    "print(f'train data size {len(dataset_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>train model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "log_r = LogisticRegression(class_weight='balanced', penalty='l1', n_jobs=-1)\n",
    "sfm = SelectFromModel(log_r, threshold=0.5)\n",
    "x = sfm.fit_transform(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=42)\n",
    "\n",
    "#sm = SMOTE(random_state=42)\n",
    "#x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', penalty='l1', n_jobs=-1) # 'penalty': ['l1', 'l2'], 'C': [1, 10, 100, 1000]\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=42)\n",
    "st=time.time()\n",
    "scores = ['f1','recall'] # 'precision', 'recall', 'f1', \n",
    "hyparam_grid = [{\n",
    "    'n_estimators': [50,100,300],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [5,10],\n",
    "    'class_weight': ['balanced'],\n",
    "}]\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(n_jobs=-1), hyparam_grid, cv=4,\n",
    "                       scoring='%s_macro' % score, n_jobs=1)\n",
    "#     clf = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=5, class_weight='balanced')\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print()\n",
    "    print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=time.time()\n",
    "time.time()-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['not lead','lead']))\n",
    "print('Acc:  {}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print('MCC: {}'.format(metrics.matthews_corrcoef(y_test, y_pred)))\n",
    "print('F1:  {}'.format(metrics.f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>submit</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfm.transform(x_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_submission = preprocess.transform(dataset_test)\n",
    "#x_submission = sfm.transform(x_submission)\n",
    "clf.fit(x, y)\n",
    "y_pred_submission = clf.predict(x_submission)\n",
    "# Creating a dictionary where the keys are the account_ids\n",
    "# and the values are your predictions\n",
    "submission_account_ids = [str(int(i))for i in dataset_test.index]\n",
    "predictions = dict(zip(submission_account_ids, map(int, y_pred_submission)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'fRidaY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We validate first that we actually send all the test accounts expected to be sent\n",
    "if y_pred_submission.shape[0] != 71683 or len(submission_account_ids) != 71683:\n",
    "  raise Exception(\"You have to send all of the accounts! Expected: (71683, 71683), Got: ({}, {})\".format(y_pred_submission.shape[0], submission_account_ids.shape[0]))\n",
    "\n",
    "if \"group_name\" not in vars() or group_name == \"\":\n",
    "  group_name = input(\"Please enter your group's name:\")\n",
    "\n",
    "data = json.dumps({'submitter': group_name, 'predictions': predictions}).encode('utf-8')\n",
    "\n",
    "req = request.Request(\"https://leaderboard.datahack.org.il/monday/api/\",\n",
    "                      headers={'Content-Type': 'application/json'},\n",
    "                      data=data)\n",
    "\n",
    "res = request.urlopen(req)\n",
    "print(json.load(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.366940718269732\n",
    "0.3697461136151904"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from urllib import request\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, Binarizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>load data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accounts = pd.read_csv(data_path + 'train_accounts.csv')#.sample(frac=0.2)\n",
    "# train_users = pd.read_csv(data_path + 'train_users.csv')\n",
    "# train_events = pd.read_csv(data_path + 'train_events.csv')\n",
    "# train_subscriptions = pd.read_csv(data_path + 'train_subscriptions.csv')\n",
    "test_accounts = pd.read_csv(data_path + 'test_accounts.csv')\n",
    "# test_users = pd.read_csv(data_path + 'test_users.csv')\n",
    "# test_events = pd.read_csv(data_path + 'test_events.csv')\n",
    "# test_subscriptions = pd.read_csv(data_path + 'test_subscriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accounts: 1433661\n"
     ]
    }
   ],
   "source": [
    "accounts = pd.concat([train_accounts, test_accounts],sort=False)\n",
    "print(f'accounts: {len(accounts)}')\n",
    "#users = pd.concat([train_users, test_users],sort=False)\n",
    "#events = pd.concat([train_events, test_events],sort=False)\n",
    "#subscriptions = pd.concat([train_subscriptions, test_subscriptions],sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>feature engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at\n",
    "churn date\n",
    "user role\n",
    "domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform plan_id & utm_cluster_id to str since its categorical\n",
    "accounts['plan_id'] = accounts['plan_id'].astype(str)\n",
    "accounts['utm_cluster_id'] = accounts['utm_cluster_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_in.loc[df_in[col_name] > fence_high, col_name] = fence_high\n",
    "    df_in.loc[df_in[col_name] < fence_low, col_name] = fence_low\n",
    "    \n",
    "def creating_time_features(data):\n",
    "    time_between_created_trial = pd.to_datetime(data['trial_start']) - pd.to_datetime(data['created_at'])\n",
    "    time_between_created_subscription = pd.to_datetime(data['subscription_started_at']) - pd.to_datetime(data['created_at'])\n",
    "    time_between_trial_subscription = pd.to_datetime(data['subscription_started_at']) - pd.to_datetime(data['trial_start'])\n",
    "    time_between_now_trial = datetime.now() - pd.to_datetime(data['trial_start'])\n",
    "    time_between_now_subscription = datetime.now() - pd.to_datetime(data['subscription_started_at'])\n",
    "    time_between_now_created = datetime.now() - pd.to_datetime(data['created_at'])\n",
    "    time_between_now_churn = datetime.now() - pd.to_datetime(data['churn_date'])\n",
    "    time_between_churn_subscription = pd.to_datetime(data['churn_date']) - pd.to_datetime(data['subscription_started_at'])\n",
    "\n",
    "    data = data.assign(created_trial_delta=time_between_created_trial.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(created_subscription_delta=time_between_created_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(trial_subscription_delta=time_between_trial_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_trial_delta=time_between_now_trial.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_subscription_delta=time_between_now_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_created_delta=time_between_now_created.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(now_churn_delta=time_between_now_churn.apply(lambda x: (x.seconds//3600)))\n",
    "    data = data.assign(churn_subscription_delta=time_between_churn_subscription.apply(lambda x: (x.seconds//3600)))\n",
    "    data['is_subscription'] = (data.subscription_started_at.isna()).astype(int)\n",
    "    data['is_churn'] = (data.churn_date.isna()).astype(int)\n",
    "    return data\n",
    "\n",
    "def creating_size_n_survey_features(data, bins, bins_labels):\n",
    "    #clip_outlier(data,'company_size')\n",
    "    data.loc[:,'avg_team_size'] = data[[\"min_team_size\", \"max_team_size\"]].mean(axis=1)\n",
    "    data['avg_team_size'].fillna(-1, inplace=True)\n",
    "    data['avg_team_cat'] = pd.cut(data['avg_team_size'], bins=bins, labels=bins_labels)\n",
    "    data['avg_team_cat'] = data['avg_team_cat'].astype(str)\n",
    "    data['survey_answers'] = data[['company_size','max_team_size','min_team_size','user_goal','user_description','team_size']].isna().sum(axis=1)\n",
    "    data['survey_did_answer'] = data['survey_answers']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating size & survey features\n",
    "bins = sorted((list(train_accounts[\"max_team_size\"].value_counts().index) + [-1.1, -1, ]))\n",
    "bins_labels = [str(b) for b in bins[1:]]\n",
    "\n",
    "accounts = creating_time_features(accounts)\n",
    "accounts = creating_size_n_survey_features(accounts, bins, bins_labels)\n",
    "accounts['country_counts'] = accounts.groupby('country')['country'].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We map our features into different types\n",
    "categorical_features = ['os', 'browser', 'payment_currency', 'device', 'country', 'industry', 'utm_cluster_id',\n",
    "                         'plan_id', 'avg_team_cat']\n",
    "normalized_features = ['collection_21_days', 'mrr', 'created_trial_delta', 'created_subscription_delta',\n",
    "                       'trial_subscription_delta', 'now_trial_delta', 'now_subscription_delta', 'now_created_delta', \n",
    "                       'now_churn_delta', 'churn_subscription_delta', 'company_size', 'survey_answers']\n",
    "normalized_features = []\n",
    "binary_features = ['survey_did_answer']\n",
    "untouched_features = ['paying', 'is_subscription', 'is_churn']\n",
    "KBinsDiscretized_features = []\n",
    "target = ['lead_score']\n",
    "\n",
    "# And create a column transformer to handle the manipulation for us\n",
    "preprocess = make_column_transformer(\n",
    "    (OneHotEncoder(), categorical_features),\n",
    "    (Normalizer(), normalized_features),\n",
    "    (Binarizer(), binary_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'account_id' in accounts.columns:\n",
    "    accounts.set_index('account_id', inplace=True)\n",
    "    #test_accounts.set_index('account_id', inplace=True)\n",
    "\n",
    "    # Filling empty values with default values \n",
    "def fill_empty_values(dataset):\n",
    "    dataset.loc[:,categorical_features] = dataset[categorical_features].fillna('')\n",
    "    dataset.loc[:,normalized_features + binary_features + untouched_features] = dataset[normalized_features + binary_features + untouched_features].fillna(0)\n",
    "    return dataset\n",
    "\n",
    "accounts = fill_empty_values(accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sample(data, target, neg_pos_ratio=1):\n",
    "    target_num = len(data[data[target] == 1])\n",
    "    negative_idx = data[data[target] == 0].index\n",
    "    positve_idx = data[data[target] == 1].index\n",
    "    rnd_negative = np.random.choice(negative_idx , target_num * neg_pos_ratio, replace=False)\n",
    "    under_sample_idx = np.concatenate([positve_idx, rnd_negative])\n",
    "    return data.loc[under_sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size 373003\n"
     ]
    }
   ],
   "source": [
    "# We fit our column transformer on both the train and the test sets\n",
    "preprocess.fit(accounts.drop('lead_score',axis=1))\n",
    "\n",
    "#dataset_train = accounts.loc[train_accounts.account_id]\n",
    "dataset_train = under_sample(accounts.loc[train_accounts.account_id], 'lead_score', 10)\n",
    "dataset_test = accounts.loc[test_accounts.account_id].drop('lead_score',axis=1)\n",
    "\n",
    "# We use transform to finally manipulate the features of our training set\n",
    "x = preprocess.transform(dataset_train.drop('lead_score',axis=1))\n",
    "# Seperating the label\n",
    "y = dataset_train['lead_score']\n",
    "print(f'train data size {len(dataset_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>train model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "log_r = LogisticRegression(class_weight='balanced', penalty='l1', n_jobs=-1)\n",
    "sfm = SelectFromModel(log_r, threshold=0.5)\n",
    "x = sfm.fit_transform(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=42)\n",
    "\n",
    "#sm = SMOTE(random_state=42)\n",
    "#x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', penalty='l1', n_jobs=-1) # 'penalty': ['l1', 'l2'], 'C': [1, 10, 100, 1000]\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05, random_state=42)\n",
    "\n",
    "scores = ['f1','recall'] # 'precision', 'recall', 'f1', \n",
    "hyparam_grid = [{\n",
    "    'n_estimators': [5, 20, 100],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 3, 5],\n",
    "    'class_weight': ['balanced'],\n",
    "}]\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(), hyparam_grid, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not lead       0.99      0.78      0.88     66428\n",
      "        lead       0.08      0.79      0.15      1672\n",
      "\n",
      "    accuracy                           0.78     68100\n",
      "   macro avg       0.54      0.79      0.52     68100\n",
      "weighted avg       0.97      0.78      0.86     68100\n",
      "\n",
      "Acc:  0.7845961820851689\n",
      "MCC: 0.2130293735382958\n",
      "F1:  0.15340220465169968\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['not lead','lead']))\n",
    "print('Acc:  {}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print('MCC: {}'.format(metrics.matthews_corrcoef(y_test, y_pred)))\n",
    "print('F1:  {}'.format(metrics.f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not lead       0.94      0.79      0.86      6813\n",
      "        lead       0.48      0.78      0.59      1665\n",
      "\n",
      "    accuracy                           0.79      8478\n",
      "   macro avg       0.71      0.79      0.72      8478\n",
      "weighted avg       0.85      0.79      0.80      8478\n",
      "\n",
      "Acc:  0.787449870252418\n",
      "MCC: 0.48565312943868216\n",
      "F1:  0.5915684496826836\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['not lead','lead']))\n",
    "print('Acc:  {}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print('MCC: {}'.format(metrics.matthews_corrcoef(y_test, y_pred)))\n",
    "print('F1:  {}'.format(metrics.f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not lead       0.80      0.79      0.80      1721\n",
      "        lead       0.79      0.80      0.79      1671\n",
      "\n",
      "    accuracy                           0.80      3392\n",
      "   macro avg       0.80      0.80      0.80      3392\n",
      "weighted avg       0.80      0.80      0.80      3392\n",
      "\n",
      "Acc:  0.7951061320754716\n",
      "MCC: 0.5903322446084891\n",
      "F1:  0.7939519715386896\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['not lead','lead']))\n",
    "print('Acc:  {}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print('MCC: {}'.format(metrics.matthews_corrcoef(y_test, y_pred)))\n",
    "print('F1:  {}'.format(metrics.f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>submit</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71683x195 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 374347 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm.transform(x_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_submission = preprocess.transform(dataset_test)\n",
    "#x_submission = sfm.transform(x_submission)\n",
    "y_pred_submission = model.predict(x_submission)\n",
    "# Creating a dictionary where the keys are the account_ids\n",
    "# and the values are your predictions\n",
    "submission_account_ids = [str(int(i))for i in dataset_test.index]\n",
    "predictions = dict(zip(submission_account_ids, map(int, y_pred_submission)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'fRidaY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 'fRidaY', 'rank': 7, 'score': 0.21192160490146092}\n"
     ]
    }
   ],
   "source": [
    "# We validate first that we actually send all the test accounts expected to be sent\n",
    "if y_pred_submission.shape[0] != 71683 or len(submission_account_ids) != 71683:\n",
    "  raise Exception(\"You have to send all of the accounts! Expected: (71683, 71683), Got: ({}, {})\".format(y_pred_submission.shape[0], submission_account_ids.shape[0]))\n",
    "\n",
    "if \"group_name\" not in vars() or group_name == \"\":\n",
    "  group_name = input(\"Please enter your group's name:\")\n",
    "\n",
    "data = json.dumps({'submitter': group_name, 'predictions': predictions}).encode('utf-8')\n",
    "\n",
    "req = request.Request(\"https://leaderboard.datahack.org.il/monday/api/\",\n",
    "                      headers={'Content-Type': 'application/json'},\n",
    "                      data=data)\n",
    "\n",
    "res = request.urlopen(req)\n",
    "print(json.load(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ['f1','recall'] # 'precision', 'recall', 'f1', \n",
    "hyparam_grid = [{\n",
    "    'n_estimators': [5, 20, 100],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 3, 5],\n",
    "    'class_weight': ['balanced'],\n",
    "}]\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(), hyparam_grid, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(x_res, y_res)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
